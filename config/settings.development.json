{
  "defaultAccounts": [
    { "email": "admin@foo.com", "password": "changeme", "role": "admin" },
    { "email": "john@foo.com", "password": "changeme" },
    { "email": "abc@foo.com", "password": "changeme" }
  ],

  "defaultTokens": [
    { "owner": "admin@foo.com", "quantity": 10 },
    { "owner": "john@foo.com", "quantity": 10 },
    { "owner": "abc@foo.com", "quantity": 10 }
  ],

  "defaultData": [
    { "name": "Basket", "quantity": 3, "owner": "john@foo.com", "condition": "excellent" },
    { "name": "Bicycle", "quantity": 2, "owner": "abc@foo.com", "condition": "poor" },
    { "name": "Banana", "quantity": 2, "owner": "admin@foo.com", "condition": "good" },
    { "name": "Boogie Board", "quantity": 2, "owner": "admin@foo.com", "condition": "excellent" }
  ],

  "defaultPaper": [
    {"title": "Applications of Cohesive Subgraph Detection", "authors": ["Dan Suthers"], "abstract": "Socio-technical networks can be productively modeled at several granularities, including the interaction of actors, how this interaction is mediated by digital artifacts, and sociograms that model direct ties between the actors themselves. Cohesive subgraph detection algorithms (CSDA, a.k.a. “community detection algorithms”) are often applied to sociograms, but also have utility in analyzing graphs corresponding to other levels of modeling. This paper illustrates applications of CSDA to graphs modeling interaction and mediated association. It reviews some leading candidate algorithms (particularly InfoMap, link communities, the Louvain method, and weakly connected components, all of which are available in R), and evaluates them with respect to how useful they have been in analyzing a large dataset derived from a network of educators known as Tapped In. This practitioner-oriented evaluation is a complement to more formal benchmark based studies common in the literature.", "area": ["HCI", "Networks"], "link": "https://scholarspace.manoa.hawaii.edu/handle/10125/41412", "owner": "john@foo.com"},
    {"title": "Algorithms and Scheduling Techniques to Manage Resilience and Power Consumption in Distributed Systems", "authors": ["Henri Casanova", "Ewa Deelman", "Yves Robert", "Uwe Schwiegelshohn"], "abstract": "Large-scale systems face two main challenges: failure management and energy management. Failure management, the goal of which is to achieve resilience, is necessary because a large number of hardware resources implies a large number of failures during the execution of an application. Energy management, the goal of which is to optimize of power consumption and to handle thermal issues, is also necessary due to both monetary and environmental constraints since typical applications executed in HPC and/or cloud environments will lead to large power consumption and heat dissipation due to intensive computation and communication workloads. The main objective of this Dagstuhl seminar was to gather two communities: (i) system- oriented researchers who study high-level resource-provisioning policies, pragmatic resource al- location and scheduling heuristics, novel approaches for designing and deploying systems software infrastructures, and tools for monitoring/measuring the state of the system; and (ii) algorithm- oriented researchers, who investigate formal models and algorithmic solutions for resilience and energy efficiency problems.", "area": ["Parallel Computing", "Workflows"], "link": "https://drops.dagstuhl.de/opus/volltexte/2016/5670/", "owner":  "admin@foo.com"},
    {"title": "Character design for soccer commentary", "authors": ["Kim Binsted"], "abstract": "In this paper we present early work on an animated talking head commentary system called Byrne. The goal of this project is to develop a system which can take the output from the RoboCup soccer simulator, and generate appropriate affective speech and facial expressions, based on the character’s personality, emotional state, and the state of play. Here we describe a system which takes pre-analysed simulator output as input, and which generates text marked-up for use by a speech generator and a face animation system. We make heavy use of inter-system standards, so that future versions of Byrne will be able to take advantage of advances in the technologies that it incorporates.", "area": ["HCI", "AI"], "link": "https://arxiv.org/abs/cmp-lg/9807012", "owner":  "abc@foo.com"},
    {"title": "Efficient Antihydrogen Detection in Antimatter Physics by Deep Learning", "authors": ["Peter Sadowski", "Balint Radics", "Ananya", "Yasunori Yamazaki", "Pierre Baldi"], "abstract": "Antihydrogen is at the forefront of antimatter research at the CERN Antiproton Decelerator. Experiments aiming to test the fundamental CPT symmetry and antigravity effects require the efficient detection of antihydrogen annihilation events, which is performed using highly granular tracking detectors installed around an antimatter trap. Improving the efficiency of the antihy- drogen annihilation detection plays a central role in the final sensitivity of the experiments. We propose deep learning as a novel technique to analyze antihydrogen annihilation data, and compare its performance with a traditional track and vertex reconstruction method. We report that the deep learning approach yields significant improvement, tripling event coverage while simultaneously improving performance by over 5% in terms of Area Under Curve (AUC).", "area": ["Physics", "Machine Learning"], "link": "https://arxiv.org/abs/1706.01826", "owner":  "john@foo.com"},
    {
      "title": "Beyond Binary Search: Parallel In-place Construction of Implicit Search Tree Layouts",
      "authors": ["Kyle Berney", "Henri Casanova", "Ben Karsin", "Nodari Sitchinava"],
      "abstract": "We present parallel algorithms to efficiently permute a sorted array into the level-order binary search tree (BST), level-order B-tree (B-tree), and van Emde Boas (vEB) layouts in-place. We analytically determine the complexity of our algorithms and empirically measure their performance. When considering the total time to permute the data in-place and to perform a series of search queries, the vEB layout provides the best performance on the CPU. Given an input of N=537 million 64-bit integers, the benefits of query performance (compared to binary search) outweigh the cost of in-place permutation when performing as few as 0.37% of N queries. On the GPU, results depend on the particular architecture, with the B-tree and vEB layouts performing the best. The number of queries necessary to reach the break-even point with binary search ranges from 1.3% to 8.9% of N=1,074 million 32-bit integers.",
      "area": ["Permutation", "Searching", "Parallel", "In-place"],
      "link": "https://henricasanova.github.io/files/papers/berney_tc2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "On the Feasibility of Simulation-driven Portfolio Scheduling for Cyberinfrastructure Runtime Systems",
      "authors": ["Henri Casanova", "Yick Ching Wong", "Loïc Pottier", "Rafael Ferreira da Silva"],
      "abstract": "Runtime systems that automate the execution of applications on distributed cyberinfrastructures need to make scheduling decisions. Researchers have proposed many scheduling algorithms, but most of them are designed based on analytical models and assumptions that may not hold in practice. The literature is thus rife with algorithms that have been evaluated only within the scope of their underlying assumptions but whose practical effectiveness is unclear. It is thus difficult for developers to decide which algorithm to implement in their runtime systems.\nTo obviate the above difficulty, we propose an approach by which the runtime system executes, throughout application execution, simulations of this very execution. Each simulation is for a different algorithm in a scheduling algorithm portfolio, and the best algorithm is selected based on simulation results. The main objective of this work is to evaluate the feasibility and potential merit of this portfolio scheduling approach, even in the presence of simulation inaccuracy, when compared to the traditional one-algorithm approach. We perform this evaluation via a case study in the context of scientific workflows. Our main finding is that portfolio scheduling can outperform the best one-algorithm approach even in the presence of relatively large simulation inaccuracies.",
      "area": ["Portfolio Scheduling", "On-line Simulation", "Workflows"],
      "link": "https://henricasanova.github.io/files/papers/casanova_jsspp2022.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "WfChef: Automated Generation of Accurate Scientific Workflow Generators",
      "authors": ["Taina Coleman", "Henri Casanova", "Rafael Ferreira da Silva"],
      "abstract": "Scientific workflow applications have become mainstream and their automated and efficient execution on largescale compute platforms is the object of extensive research and development. For these efforts to be successful, a solid experimental methodology is needed to evaluate workflow algorithms and systems. A foundation for this methodology is the availability of realistic workflow instances. Dozens of workflow instances for a few scientific applications are available in public repositories. While these are invaluable, they are limited: workflow instances are not available for all application scales of interest. To address this limitation, previous work has developed generators of synthetic, but representative, workflow instances of arbitrary scales. These generators are popular, but implementing them is a manual, labor-intensive process that requires expert application knowledge. As a result, these generators only target a handful of applications, even though hundreds of applications use workflows in production.\nIn this work, we present WfChef, a framework that fully automates the process of constructing a synthetic workflow generator for any scientific application. Based on an input set of workflow instances, WfChef automatically produces a synthetic workflow generator. We define and evaluate several metrics for quantifying the realism of the generated workflows. Using these metrics, we compare the realism of the workflows generated by WfChef generators to that of the workflows generated by the previously available, hand-crafted generators. We find that the WfChef generators not only require zero development effort (because it is automatically produced), but also generate workflows that are more realistic than those generated by hand-crafted generators.",
      "area": ["Scientific workflows", "Synthetic workflow generation", "Workflow management systems"],
      "link": "https://henricasanova.github.io/files/papers/coleman_escience2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Teaching Parallel and Distributed Computing Concepts in Simulation with WRENCH",
      "authors": ["Henri Casanova", "Ryan Tanaka", "William Koch", "Rafael Ferreira da Silva"],
      "abstract": "Teaching parallel and distributed computing topics in a hands-on manner is challenging, especially at introductory, undergraduate levels. Participation challenges arise due to the need to provide students with an appropriate compute platform, which is not always possible. Even if a platform is provided to students, not all relevant learning objectives can be achieved via hands-on learning on a single platform. In particular, it is typically not feasible to provide students with platform configurations representative of emerging and future cyberinfrastructure scenarios (e.g., highly distributed, heterogeneous platforms with large numbers of high-end compute nodes). To address these challenges, we have developed a set of pedagogic modules that can be integrated piecemeal into university courses. These modules include simulation-driven activities for students to experience relevant application and platform scenarios hands-on. These activities are supported by simulators developed using the WRENCH simulation framework. After motivating and describing our approach, we present and analyze results obtained from evaluations performed in two consecutive offerings of an undergraduate university course.",
      "area": ["Computer Science Education", "Parallel and Distributed Computing Education", "Simulation"],
      "link": "https://henricasanova.github.io/files/papers/casanova_jpdc2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Evaluating energy-aware scheduling algorithms for I/O-intensive scientific workflows",
      "authors": ["Taina Coleman", "Henri Casanova", "Ty Gwartney", "Rafael Ferreira da Silva"],
      "abstract": "Improving energy efficiency has become necessary to enable sustainable computational science. At the same time, scientific workflows are key in facilitating distributed computing in virtually all domain sciences. As data and computational requirements increase, I/O-intensive workflows have become prevalent. In this work, we evaluate the ability of two popular energy-aware workflow scheduling algorithms to provide effective schedules for this class of workflow applications, that is, schedules that strike a good compromise between workflow execution time and energy consumption. These two algorithms make decisions based on a widely used power consumption model that simply assumes linear correlation to CPU usage. Previous work has shown this model to be inaccurate, in particular for modeling power consumption of I/O-intensive workflow executions, and has proposed an accurate model. We evaluate the effectiveness of the two aforementioned algorithms based on this accurate model. We find that, when making their decisions, these algorithms can underestimate power consumption by up to 360%, which makes it unclear how well these algorithm would fare in practice. To evaluate the benefit of using the more accurate power consumption model in practice, we propose a simple scheduling algorithm that relies on this model to balance the I/O load across the available compute resources. Experimental results show that this algorithm achieves more desirable compromises between energy consumption and workflow execution time than the two popular algorithms.",
      "area": ["Scientific workflows", "Energy-aware computing", "Workflow scheduling", "Workflow simulation"],
      "link": "https://henricasanova.github.io/files/papers/coleman_iccs2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "GLUME: A Strategy for Reducing Workflow Execution Times on Batch-Scheduled Platforms",
      "authors": ["Evan Hataishi", "Pierre-Francois Dutot", "Rafael Ferreira da Silva", "Henri Casanova"],
      "abstract": "Many scientific workflows have computational demands that require the use of compute platforms managed by batch schedulers, which are unfortunately poorly suited to these applications. This work proposes GLUME, a strategy for partitioning a workflow into batch jobs. The novelty is that these jobs are explicitly constructed to minimize overall workflow execution time. Experimental evaluation via simulation of production batch workloads and workflows shows that our heuristic is more effective than previously proposed strategies when executing workflows with moderate to high computational demand.",
      "area": ["Batch Scheduling", "Workflows", "Task Clustering"],
      "link": "https://henricasanova.github.io/files/papers/hataishi_jsspp2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Peachy Parallel Assignments (EduHPC 2020)",
      "authors": ["Henri Casanova", "Rafael Ferreira da Silva", "Arturo Gonzalez-Escribano", "William Koch", "Yuri Torres", "David P. Bunde"],
      "abstract": "Peachy Parallel Assignments are high-quality assignments for teaching parallel and distributed computing. They are selected competitively for presentation at the Edu* workshops. All of the assignments have been successfully used in class and they are selected based on the their ease of adoption by other instructors and for being cool and inspirational to students. This paper presents a paper-and-pencil assignment asking students to analyze the performance of different system configurations and an assignment in which students parallelize a simulation of the evolution of simple living organisms.",
      "area": ["Peachy Parallel Assignments", "Parallel computing education", "High-Performance Computing education", ", Parallel programming", "Curriculum Development", "Performance analysis", "Parallel simulation", "OpenMP", "MPI", "GPGPU"],
      "link": "https://henricasanova.github.io/files/papers/casanova_eduhpc2020.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "WorkflowHub: Community Framework for Enabling Scientific Workflow Research and Development",
      "authors": ["Rafael Ferreira da Silva", "Loïc Pottier", "Taina Coleman", "Ewa Deelman", "Henri Casanova"],
      "abstract": "Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous work, we have presented a collection of tools that have been used for aiding research and development activities in the Pegasus project, and that have been adopted by others for conducting workflow research. Despite their popularity, there are several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present WorkflowHub, a community framework that provides a collection of tools for analyzing workflow execution traces, producing realistic synthetic workflow traces, and simulating workflow executions. We demonstrate the realism of the generated synthetic traces by comparing simulated executions of these traces with actual workflow executions. We also contrast these results with those obtained when using the previously available collection of tools. We find that our framework not only can be used to generate representative synthetic workflow traces (i.e., with workflow structures and task characteristics distributions that resemble those in traces obtained from real-world workflow executions), but can also generate representative workflow.",
      "area": ["Scientific Workflows", "Workflow Management Systems", "Simulation, Distributed Computing", "Workflow Traces"],
      "link": "https://henricasanova.github.io/files/papers/ferreiradasilva_works2020.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Modeling the Performance of Scientific Workflow Executions on HPC Platforms with Burst Buffers",
      "authors": ["Loïc Pottier", "Rafael Ferreira da Silva", "Henri Casanova", "Ewa Deelman"],
      "abstract": "Scientific domains ranging from bioinformatics to astronomy and earth science rely on traditional high-performance computing (HPC) codes, often encapsulated in scientific workflows. In contrast to traditional HPC codes that employ a few programming and runtime approaches that are highly optimized for HPC platforms, scientific workflows are not necessarily optimized for these platforms. As an effort to reduce the gap between compute and I/O performance, HPC platforms have adopted intermediate storage layers known as burst buffers. A burst buffer (BB) is a fast storage layer positioned between the global parallel file system and the compute nodes. Two designs currently exist: (i) shared, where the BBs are located on dedicated nodes; and (ii) on-node, in which each compute node embeds a private BB. In this paper, using accurate simulations and realworld experiments, we study how to best use these new storage layers when executing scientific workflows. These applications are not necessarily optimized to run on HPC systems, and thus can exhibit I/O patterns that differ from that of HPC codes. Thus, we first characterize the I/O behaviors of a real-world workflow under different configuration scenarios on two leadership-class HPC systems (Cori at NERSC and Summit at ORNL). Then, we use these characterizations to calibrate a simulator for workflow executions on HPC systems featuring shared and private BBs. Last, we evaluate our approach against a large I/O-intensive workflow, and we provide insights on the performance levels and the potential limitations of these two BBs architectures.",
      "area": ["Scientific workflow", "Simulations", "Burst buffers", "High-Performance Computing", "Performance Characterization"],
      "link": "https://henricasanova.github.io/files/papers/pottier_cluster2020.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Characterizing, Modeling, and Accurately Simulating Power and Energy Consumption of I/O-intensive Scientific Workflows",
      "authors": ["Rafael Ferreira da Silva", "Henri Casanova", "Anne-Cecile Orgerie", "Ryan Tanaka", "Ewa Deelman", "Frederic Suter"],
      "abstract": "While distributed computing infrastructures can provide infrastructure-level techniques for managing energy consumption, application-level energy consumption models have also been developed to support energy-efficient scheduling and resource provisioning algorithms. In this work, we analyze the accuracy of a widely-used application-level model that has been developed and used in the context of scientific workflow executions. To this end, we profile two production scientific workflows on a distributed platform instrumented with power meters. We then conduct an analysis of power and energy consumption measurements. This analysis shows that power consumption is not linearly related to CPU utilization and that I/O operations significantly impact power, and thus energy, consumption. We then propose a power consumption model that accounts for I/O operations, including the impact of waiting for these operations to complete, and for concurrent task executions on multi-socket, multi-core compute nodes. We implement our proposed model as part of a simulator that allows us to draw direct comparisons between real-world and modeled power and energy consumption. We find that our model has high accuracy when compared to real-world executions. Furthermore, our model improves accuracy by about two orders of magnitude when compared to the traditional models used in the energy-efficient workflow scheduling literature.",
      "area": ["Scientific workflows", "Energy-aware computing", "Workflow profiling", "Workflow scheduling"],
      "link": "https://henricasanova.github.io/files/papers/ferreiradasilva_jocs2020.pdf",
      "owner": "john@foo.com"
    }
  ]
}
