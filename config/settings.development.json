{
  "defaultAccounts": [
    { "email": "admin@foo.com", "password": "changeme", "role": "admin" },
    { "email": "john@foo.com", "password": "changeme" },
    { "email": "abc@foo.com", "password": "changeme" },
    { "email": "xyz@foo.com", "password": "changeme" }
  ],

  "defaultTokens": [
    { "owner": "admin@foo.com", "quantity": 10 },
    { "owner": "john@foo.com", "quantity": 10 },
    { "owner": "abc@foo.com", "quantity": 10 }
  ],

  "defaultData": [
    { "name": "Basket", "quantity": 3, "owner": "john@foo.com", "condition": "excellent" },
    { "name": "Bicycle", "quantity": 2, "owner": "abc@foo.com", "condition": "poor" },
    { "name": "Banana", "quantity": 2, "owner": "admin@foo.com", "condition": "good" },
    { "name": "Boogie Board", "quantity": 2, "owner": "admin@foo.com", "condition": "excellent" }
  ],

  "defaultPaper": [
    {"title": "Applications of Cohesive Subgraph Detection", "authors": ["Dan Suthers"], "abstract": "Socio-technical networks can be productively modeled at several granularities, including the interaction of actors, how this interaction is mediated by digital artifacts, and sociograms that model direct ties between the actors themselves. Cohesive subgraph detection algorithms (CSDA, a.k.a. “community detection algorithms”) are often applied to sociograms, but also have utility in analyzing graphs corresponding to other levels of modeling. This paper illustrates applications of CSDA to graphs modeling interaction and mediated association. It reviews some leading candidate algorithms (particularly InfoMap, link communities, the Louvain method, and weakly connected components, all of which are available in R), and evaluates them with respect to how useful they have been in analyzing a large dataset derived from a network of educators known as Tapped In. This practitioner-oriented evaluation is a complement to more formal benchmark based studies common in the literature.", "area": ["HCI", "Networks"], "link": "https://scholarspace.manoa.hawaii.edu/handle/10125/41412", "owner": "john@foo.com"},
    {"title": "Algorithms and Scheduling Techniques to Manage Resilience and Power Consumption in Distributed Systems", "authors": ["Henri Casanova", "Ewa Deelman", "Yves Robert", "Uwe Schwiegelshohn"], "abstract": "Large-scale systems face two main challenges: failure management and energy management. Failure management, the goal of which is to achieve resilience, is necessary because a large number of hardware resources implies a large number of failures during the execution of an application. Energy management, the goal of which is to optimize of power consumption and to handle thermal issues, is also necessary due to both monetary and environmental constraints since typical applications executed in HPC and/or cloud environments will lead to large power consumption and heat dissipation due to intensive computation and communication workloads. The main objective of this Dagstuhl seminar was to gather two communities: (i) system- oriented researchers who study high-level resource-provisioning policies, pragmatic resource al- location and scheduling heuristics, novel approaches for designing and deploying systems software infrastructures, and tools for monitoring/measuring the state of the system; and (ii) algorithm- oriented researchers, who investigate formal models and algorithmic solutions for resilience and energy efficiency problems.", "area": ["Parallel Computing", "Workflows"], "link": "https://drops.dagstuhl.de/opus/volltexte/2016/5670/", "owner":  "admin@foo.com"},
    {"title": "Character design for soccer commentary", "authors": ["Kim Binsted"], "abstract": "In this paper we present early work on an animated talking head commentary system called Byrne. The goal of this project is to develop a system which can take the output from the RoboCup soccer simulator, and generate appropriate affective speech and facial expressions, based on the character’s personality, emotional state, and the state of play. Here we describe a system which takes pre-analysed simulator output as input, and which generates text marked-up for use by a speech generator and a face animation system. We make heavy use of inter-system standards, so that future versions of Byrne will be able to take advantage of advances in the technologies that it incorporates.", "area": ["HCI", "AI"], "link": "https://arxiv.org/abs/cmp-lg/9807012", "owner":  "abc@foo.com"},
    {"title": "Efficient Antihydrogen Detection in Antimatter Physics by Deep Learning", "authors": ["Peter Sadowski", "Balint Radics", "Ananya", "Yasunori Yamazaki", "Pierre Baldi"], "abstract": "Antihydrogen is at the forefront of antimatter research at the CERN Antiproton Decelerator. Experiments aiming to test the fundamental CPT symmetry and antigravity effects require the efficient detection of antihydrogen annihilation events, which is performed using highly granular tracking detectors installed around an antimatter trap. Improving the efficiency of the antihy- drogen annihilation detection plays a central role in the final sensitivity of the experiments. We propose deep learning as a novel technique to analyze antihydrogen annihilation data, and compare its performance with a traditional track and vertex reconstruction method. We report that the deep learning approach yields significant improvement, tripling event coverage while simultaneously improving performance by over 5% in terms of Area Under Curve (AUC).", "area": ["Physics", "Machine Learning"], "link": "https://arxiv.org/abs/1706.01826", "owner":  "john@foo.com"},
    {
      "title": "Beyond Binary Search: Parallel In-place Construction of Implicit Search Tree Layouts",
      "authors": ["Kyle Berney", "Henri Casanova", "Ben Karsin", "Nodari Sitchinava"],
      "abstract": "We present parallel algorithms to efficiently permute a sorted array into the level-order binary search tree (BST), level-order B-tree (B-tree), and van Emde Boas (vEB) layouts in-place. We analytically determine the complexity of our algorithms and empirically measure their performance. When considering the total time to permute the data in-place and to perform a series of search queries, the vEB layout provides the best performance on the CPU. Given an input of N=537 million 64-bit integers, the benefits of query performance (compared to binary search) outweigh the cost of in-place permutation when performing as few as 0.37% of N queries. On the GPU, results depend on the particular architecture, with the B-tree and vEB layouts performing the best. The number of queries necessary to reach the break-even point with binary search ranges from 1.3% to 8.9% of N=1,074 million 32-bit integers.",
      "area": ["Permutation", "Searching", "Parallel", "In-place"],
      "link": "https://henricasanova.github.io/files/papers/berney_tc2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "On the Feasibility of Simulation-driven Portfolio Scheduling for Cyberinfrastructure Runtime Systems",
      "authors": ["Henri Casanova", "Yick Ching Wong", "Loïc Pottier", "Rafael Ferreira da Silva"],
      "abstract": "Runtime systems that automate the execution of applications on distributed cyberinfrastructures need to make scheduling decisions. Researchers have proposed many scheduling algorithms, but most of them are designed based on analytical models and assumptions that may not hold in practice. The literature is thus rife with algorithms that have been evaluated only within the scope of their underlying assumptions but whose practical effectiveness is unclear. It is thus difficult for developers to decide which algorithm to implement in their runtime systems. To obviate the above difficulty, we propose an approach by which the runtime system executes, throughout application execution, simulations of this very execution. Each simulation is for a different algorithm in a scheduling algorithm portfolio, and the best algorithm is selected based on simulation results. The main objective of this work is to evaluate the feasibility and potential merit of this portfolio scheduling approach, even in the presence of simulation inaccuracy, when compared to the traditional one-algorithm approach. We perform this evaluation via a case study in the context of scientific workflows. Our main finding is that portfolio scheduling can outperform the best one-algorithm approach even in the presence of relatively large simulation inaccuracies.",
      "area": ["Portfolio Scheduling", "On-line Simulation", "Workflows"],
      "link": "https://henricasanova.github.io/files/papers/casanova_jsspp2022.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "WfChef: Automated Generation of Accurate Scientific Workflow Generators",
      "authors": ["Taina Coleman", "Henri Casanova", "Rafael Ferreira da Silva"],
      "abstract": "Scientific workflow applications have become mainstream and their automated and efficient execution on largescale compute platforms is the object of extensive research and development. For these efforts to be successful, a solid experimental methodology is needed to evaluate workflow algorithms and systems. A foundation for this methodology is the availability of realistic workflow instances. Dozens of workflow instances for a few scientific applications are available in public repositories. While these are invaluable, they are limited: workflow instances are not available for all application scales of interest. To address this limitation, previous work has developed generators of synthetic, but representative, workflow instances of arbitrary scales. These generators are popular, but implementing them is a manual, labor-intensive process that requires expert application knowledge. As a result, these generators only target a handful of applications, even though hundreds of applications use workflows in production. In this work, we present WfChef, a framework that fully automates the process of constructing a synthetic workflow generator for any scientific application. Based on an input set of workflow instances, WfChef automatically produces a synthetic workflow generator. We define and evaluate several metrics for quantifying the realism of the generated workflows. Using these metrics, we compare the realism of the workflows generated by WfChef generators to that of the workflows generated by the previously available, hand-crafted generators. We find that the WfChef generators not only require zero development effort (because it is automatically produced), but also generate workflows that are more realistic than those generated by hand-crafted generators.",
      "area": ["Scientific workflows", "Synthetic workflow generation", "Workflow management systems"],
      "link": "https://henricasanova.github.io/files/papers/coleman_escience2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Teaching Parallel and Distributed Computing Concepts in Simulation with WRENCH",
      "authors": ["Henri Casanova", "Ryan Tanaka", "William Koch", "Rafael Ferreira da Silva"],
      "abstract": "Teaching parallel and distributed computing topics in a hands-on manner is challenging, especially at introductory, undergraduate levels. Participation challenges arise due to the need to provide students with an appropriate compute platform, which is not always possible. Even if a platform is provided to students, not all relevant learning objectives can be achieved via hands-on learning on a single platform. In particular, it is typically not feasible to provide students with platform configurations representative of emerging and future cyberinfrastructure scenarios (e.g., highly distributed, heterogeneous platforms with large numbers of high-end compute nodes). To address these challenges, we have developed a set of pedagogic modules that can be integrated piecemeal into university courses. These modules include simulation-driven activities for students to experience relevant application and platform scenarios hands-on. These activities are supported by simulators developed using the WRENCH simulation framework. After motivating and describing our approach, we present and analyze results obtained from evaluations performed in two consecutive offerings of an undergraduate university course.",
      "area": ["Computer Science Education", "Parallel and Distributed Computing Education", "Simulation"],
      "link": "https://henricasanova.github.io/files/papers/casanova_jpdc2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Evaluating energy-aware scheduling algorithms for I/O-intensive scientific workflows",
      "authors": ["Taina Coleman", "Henri Casanova", "Ty Gwartney", "Rafael Ferreira da Silva"],
      "abstract": "Improving energy efficiency has become necessary to enable sustainable computational science. At the same time, scientific workflows are key in facilitating distributed computing in virtually all domain sciences. As data and computational requirements increase, I/O-intensive workflows have become prevalent. In this work, we evaluate the ability of two popular energy-aware workflow scheduling algorithms to provide effective schedules for this class of workflow applications, that is, schedules that strike a good compromise between workflow execution time and energy consumption. These two algorithms make decisions based on a widely used power consumption model that simply assumes linear correlation to CPU usage. Previous work has shown this model to be inaccurate, in particular for modeling power consumption of I/O-intensive workflow executions, and has proposed an accurate model. We evaluate the effectiveness of the two aforementioned algorithms based on this accurate model. We find that, when making their decisions, these algorithms can underestimate power consumption by up to 360%, which makes it unclear how well these algorithm would fare in practice. To evaluate the benefit of using the more accurate power consumption model in practice, we propose a simple scheduling algorithm that relies on this model to balance the I/O load across the available compute resources. Experimental results show that this algorithm achieves more desirable compromises between energy consumption and workflow execution time than the two popular algorithms.",
      "area": ["Scientific workflows", "Energy-aware computing", "Workflow scheduling", "Workflow simulation"],
      "link": "https://henricasanova.github.io/files/papers/coleman_iccs2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "GLUME: A Strategy for Reducing Workflow Execution Times on Batch-Scheduled Platforms",
      "authors": ["Evan Hataishi", "Pierre-Francois Dutot", "Rafael Ferreira da Silva", "Henri Casanova"],
      "abstract": "Many scientific workflows have computational demands that require the use of compute platforms managed by batch schedulers, which are unfortunately poorly suited to these applications. This work proposes GLUME, a strategy for partitioning a workflow into batch jobs. The novelty is that these jobs are explicitly constructed to minimize overall workflow execution time. Experimental evaluation via simulation of production batch workloads and workflows shows that our heuristic is more effective than previously proposed strategies when executing workflows with moderate to high computational demand.",
      "area": ["Batch Scheduling", "Workflows", "Task Clustering"],
      "link": "https://henricasanova.github.io/files/papers/hataishi_jsspp2021.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Peachy Parallel Assignments (EduHPC 2020)",
      "authors": ["Henri Casanova", "Rafael Ferreira da Silva", "Arturo Gonzalez-Escribano", "William Koch", "Yuri Torres", "David P. Bunde"],
      "abstract": "Peachy Parallel Assignments are high-quality assignments for teaching parallel and distributed computing. They are selected competitively for presentation at the Edu* workshops. All of the assignments have been successfully used in class and they are selected based on the their ease of adoption by other instructors and for being cool and inspirational to students. This paper presents a paper-and-pencil assignment asking students to analyze the performance of different system configurations and an assignment in which students parallelize a simulation of the evolution of simple living organisms.",
      "area": ["Peachy Parallel Assignments", "Parallel computing education", "High-Performance Computing education", ", Parallel programming", "Curriculum Development", "Performance analysis", "Parallel simulation", "OpenMP", "MPI", "GPGPU"],
      "link": "https://henricasanova.github.io/files/papers/casanova_eduhpc2020.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "WorkflowHub: Community Framework for Enabling Scientific Workflow Research and Development",
      "authors": ["Rafael Ferreira da Silva", "Loïc Pottier", "Taina Coleman", "Ewa Deelman", "Henri Casanova"],
      "abstract": "Scientific workflows are a cornerstone of modern scientific computing. They are used to describe complex computational applications that require efficient and robust management of large volumes of data, which are typically stored/processed on heterogeneous, distributed resources. The workflow research and development community has employed a number of methods for the quantitative evaluation of existing and novel workflow algorithms and systems. In particular, a common approach is to simulate workflow executions. In previous work, we have presented a collection of tools that have been used for aiding research and development activities in the Pegasus project, and that have been adopted by others for conducting workflow research. Despite their popularity, there are several shortcomings that prevent easy adoption, maintenance, and consistency with the evolving structures and computational requirements of production workflows. In this work, we present WorkflowHub, a community framework that provides a collection of tools for analyzing workflow execution traces, producing realistic synthetic workflow traces, and simulating workflow executions. We demonstrate the realism of the generated synthetic traces by comparing simulated executions of these traces with actual workflow executions. We also contrast these results with those obtained when using the previously available collection of tools. We find that our framework not only can be used to generate representative synthetic workflow traces (i.e., with workflow structures and task characteristics distributions that resemble those in traces obtained from real-world workflow executions), but can also generate representative workflow.",
      "area": ["Scientific Workflows", "Workflow Management Systems", "Simulation, Distributed Computing", "Workflow Traces"],
      "link": "https://henricasanova.github.io/files/papers/ferreiradasilva_works2020.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Modeling the Performance of Scientific Workflow Executions on HPC Platforms with Burst Buffers",
      "authors": ["Loïc Pottier", "Rafael Ferreira da Silva", "Henri Casanova", "Ewa Deelman"],
      "abstract": "Scientific domains ranging from bioinformatics to astronomy and earth science rely on traditional high-performance computing (HPC) codes, often encapsulated in scientific workflows. In contrast to traditional HPC codes that employ a few programming and runtime approaches that are highly optimized for HPC platforms, scientific workflows are not necessarily optimized for these platforms. As an effort to reduce the gap between compute and I/O performance, HPC platforms have adopted intermediate storage layers known as burst buffers. A burst buffer (BB) is a fast storage layer positioned between the global parallel file system and the compute nodes. Two designs currently exist: (i) shared, where the BBs are located on dedicated nodes; and (ii) on-node, in which each compute node embeds a private BB. In this paper, using accurate simulations and realworld experiments, we study how to best use these new storage layers when executing scientific workflows. These applications are not necessarily optimized to run on HPC systems, and thus can exhibit I/O patterns that differ from that of HPC codes. Thus, we first characterize the I/O behaviors of a real-world workflow under different configuration scenarios on two leadership-class HPC systems (Cori at NERSC and Summit at ORNL). Then, we use these characterizations to calibrate a simulator for workflow executions on HPC systems featuring shared and private BBs. Last, we evaluate our approach against a large I/O-intensive workflow, and we provide insights on the performance levels and the potential limitations of these two BBs architectures.",
      "area": ["Scientific workflow", "Simulations", "Burst buffers", "High-Performance Computing", "Performance Characterization"],
      "link": "https://henricasanova.github.io/files/papers/pottier_cluster2020.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Characterizing, Modeling, and Accurately Simulating Power and Energy Consumption of I/O-intensive Scientific Workflows",
      "authors": ["Rafael Ferreira da Silva", "Henri Casanova", "Anne-Cecile Orgerie", "Ryan Tanaka", "Ewa Deelman", "Frederic Suter"],
      "abstract": "While distributed computing infrastructures can provide infrastructure-level techniques for managing energy consumption, application-level energy consumption models have also been developed to support energy-efficient scheduling and resource provisioning algorithms. In this work, we analyze the accuracy of a widely-used application-level model that has been developed and used in the context of scientific workflow executions. To this end, we profile two production scientific workflows on a distributed platform instrumented with power meters. We then conduct an analysis of power and energy consumption measurements. This analysis shows that power consumption is not linearly related to CPU utilization and that I/O operations significantly impact power, and thus energy, consumption. We then propose a power consumption model that accounts for I/O operations, including the impact of waiting for these operations to complete, and for concurrent task executions on multi-socket, multi-core compute nodes. We implement our proposed model as part of a simulator that allows us to draw direct comparisons between real-world and modeled power and energy consumption. We find that our model has high accuracy when compared to real-world executions. Furthermore, our model improves accuracy by about two orders of magnitude when compared to the traditional models used in the energy-efficient workflow scheduling literature.",
      "area": ["Scientific workflows", "Energy-aware computing", "Workflow profiling", "Workflow scheduling"],
      "link": "https://henricasanova.github.io/files/papers/ferreiradasilva_jocs2020.pdf",
      "owner": "john@foo.com"
    },
    {
      "title": "Developing Accurate and Scalable Simulators of Production Workflow Management Systems with WRENCH",
      "authors": ["Henri Casanova", "Rafael Ferreira da Silva", "Ryan Tanaka", "Suraj Pandey", "Gautam Jethwani", "William Koch", "Spencer Albrecht", "James Oeth", "Frederic Suter"],
      "abstract": "Scientific workflows are used routinely in numerous scientific domains, and Workflow Management Systems (WMSs) have been developed to orchestrate and optimize workflow executions on distributed platforms. WMSs are complex software systems that interact with complex software infrastructures. Most WMS research and development activities rely on empirical experiments conducted with full-fledged software stacks on actual hardware platforms. These experiments, however, are limited to hardware and software infrastructures at hand and can be labor and/or time-intensive. As a result, relying solely on real-world experiments impedes WMS research and development. An alternative is to conduct experiments in simulation. In this work we present WRENCH, a WMS simulation framework, whose objectives are (i) accurate and scalable simulations; and (ii) easy simulation software development. WRENCH achieves its first objective by building on the SimGrid framework. While SimGrid is recognized for the accuracy and scalability of its simulation models, it only provides low-level simulation abstractions and thus large software development efforts are required when implementing simulators of complex systems. WRENCH thus achieves its second objective by providing high-level and directly re-usable simulation abstractions on top of SimGrid. After describing and giving rationales for WRENCH’s software architecture and APIs, we present two case studies in which we apply WRENCH to simulate the Pegasus production WMS and the WorkQueue application execution framework. We report on ease of implementation, simulation accuracy, and simulation scalability so as to determine to which extent WRENCH achieves its objectives. We also draw both qualitative and quantitative comparisons with a previously proposed workflow simulator.",
      "area": ["Scientific Workflows", "Workflow Management Systems", "Simulation", "Distributed Computing"],
      "link": "https://henricasanova.github.io/files/papers/casanova_fgcs2020.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "Bridging Concepts and Practice in eScience via Simulation-driven Engineering",
      "authors": ["Rafael Ferreira da Silva", "Henri Casanova", "Ryan Tanaka", "Frederic Suter"],
      "abstract": "The CyberInfrastructure (CI) has been the object of intensive research and development in the last decade, resulting in a rich set of abstractions and interoperable software implementations that are used in production today for supporting ongoing and breakthrough scientific discoveries. A key challenge is the development of tools and application execution frameworks that are robust in current and emerging CI configurations, and that can anticipate the needs of upcoming CI applications. This paper presents WRENCH, a framework that enables simulation-driven engineering for evaluating and developing CI application execution frameworks. WRENCH provides a set of high-level simulation abstractions that serve as building blocks for developing custom simulators. These abstractions rely on the scalable and accurate simulation models that are provided by the SimGrid simulation framework. Consequently, WRENCH makes it possible to build, with minimum software development effort, simulators that that can accurately and scalably simulate a wide spectrum of large and complex CI scenarios. These simulators can then be used to evaluate and/or compare alternate platform, system, and algorithm designs, so as to drive the development of CI solutions for current and emerging applications.",
      "area": ["CyberInfrastrucutre Development", "Simulation Accuracy", "Reproducible Research", "Distributed Computing"],
      "link": "https://henricasanova.github.io/files/papers/ferreiradasilva_escience2019.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "MILP Formulations for Spatio-Temporal Thermal-Aware Scheduling in Cloud and HPC Datacenters",
      "authors": ["Jean-Marc Pierson", "Patricia Stolf", "Hongyang Sun", "Henri Casanova"],
      "abstract": "This paper focuses on scheduling problems related to the execution of computational jobs in datacenters with thermal constraints. Mixed integer linear programming (MILP) formulations are proposed that encompass both spatial and temporal aspects of the temperature evolution under a unified model. This model takes into account the dynamics of heat production and dissipation in order to schedule jobs at appropriate times on appropriate machines. The proposed MILP formulations are applicable to both high-performance computing (HPC) and Cloud settings, and can target several objectives including energy and makespan minimization, while incorporating the cooling costs and dynamic voltage and frequency scaling (DVFS) capabilities of servers. The applicability and usefulness of our formulations are demonstrated via several HPC and Cloud case-studies.",
      "area": ["HPC and Cloud datacenters", "Thermal modeling", "Thermal-aware scheduling", "Makespan", "Energy consumption", "Linear programming"],
      "link": "https://henricasanova.github.io/files/papers/pierson_cluster2019.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "Accurately Simulating Energy Consumption of I/O-intensive Scientific Workflows",
      "authors": ["Rafael Ferreira da Silva", "Anne-Cecile Orgerie", "Henri Casanova","Ryan Tanaka", "Ewa Deelman", "Frederic Suter"],
      "abstract": "While distributed computing infrastructures can provide infrastructure-level techniques for managing energy consumption, application-level energy consumption models have also been developed to support energy-efficient scheduling and resource provisioning algorithms. In this work, we analyze the accuracy of a widely-used application-level model that have been developed and used in the context of scientific workflow executions. To this end, we profile two production scientific workflows on a distributed platform instrumented with power meters. We then conduct an analysis of power and energy consumption measurements. This analysis shows that power consumption is not linearly related to CPU utilization and that I/O operations significantly impact power, and thus energy, consumption. We then propose a power consumption model that accounts for I/O operations, including the impact of waiting for these operations to complete, and for concurrent task executions on multi-socket, multi-core compute nodes. We implement our proposed model as part of a simulator that allows us to draw direct comparisons between real-world and modeled power and energy consumption. We find that our model has high accuracy when compared to real-world executions. Furthermore, our model improves accuracy by about two orders of magnitude when compared to the traditional models used in the energy efficient workflow scheduling literature.",
      "area": ["Scientific workflows", "Energy-aware computing", "Workflow profiling", "Workflow scheduling"],
      "link": "https://henricasanova.github.io/files/papers/da_silva_iccs2019.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "Sparse 3-D NoCs with Inductive Coupling",
      "authors": ["Michihiro Koibuchi", "Lambert Leong", "Tomohiro Totoki", "Naoya Niwa", "Hiroki Matsutani", "Hideharu Amano", "Henri Casanova"],
      "abstract": "Wireless interconnects based on inductive coupling technology are compelling propositions for designing 3-D integrated chips. This work addresses the heat dissipation problem on such systems. Although effective cooling technologies have been proposed for systems designed based on Through Silicon Via (TSV), their application to systems that use inductive coupling is problematic because of increased wireless-communication distance. For this reason, we propose two methods for designing sparse 3-D chips layouts and Networks on Chip (NoCs) based on inductive coupling. The first method computes an optimized 3-D chip layout and then generates a randomized network topology for this layout. The second method uses a standard stack chip layout with a standard network topology as a starting point, and then deterministically transforms it into either a “staircase” or a “checkerboard” layout. We quantitatively compare the designs produced by these two methods in terms of network and application performance. Our main finding is that the first method produces designs that ultimately lead to higher parallel application performance, as demonstrated for nine OpenMP applications in the NAS Parallel Benchmarks.",
      "area": ["Inductive coupling", "3-D chip layout"],
      "link": "https://henricasanova.github.io/files/papers/koibuchi_dac2019.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "An Efficient Algorithm for the 1D Total Visibility-Index Problem",
      "authors": ["Peyman Afshani", "Mark de Berg", "Henri Casanova", "Benjamin Karsin", "Colin Lambrechts", "Nodari Sitchinava", "Constantinos Tsirogiannis"],
      "abstract": "Let T be a terrain and P be a set of points on its surface. An important problem in Geographic Information Science (GIS) is computing the visibility index of a point p on P, that is, the number of points in P that are visible from p. The total visibility-index problem asks for the visibility index of every point in P. We present the first subquadratic-time algorithm to solve the 1D total-visibility-index problem. Our algorithm uses a geometric dualization technique to reduce the problem to a set of instances of the red-blue line segment intersection counting problem, allowing us to find the total visibility-index in O (n log2 n) time. We implement a naive O (n2 ) approach and four variations of our algorithm: one that uses an existing red-blue line segment intersection counting algorithm and three new approaches that leverage features specific to our problem. Two of our implementations allow for parallel execution, requiring O (log2 n) time and O (n log2 n) work in the CREW PRAM model. We present experimental results for both serial and parallel implementations on synthetic and real-world datasets, using two hardware platforms. Results show that all variants of our algorithm outperform the naive approach by several orders of magnitude. Furthermore, we show that our special-case red-blue line segment intersection counting implementations out-perform the existing general-case solution by up to a factor 10. Our fastest parallel implementation is able to process a terrain of more than 100 million vertices in under 3 minutes, achieving up to 85% parallel efficiency using 16 cores.",
      "area": ["Theory of computation", "Computational geometry", "Data structures design and analysis", "Parallel algorithms",
        "Divide and conquer"],
      "link": "https://henricasanova.github.io/files/papers/afshani_jea2018.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "Analysis-driven Engineering of Comparison-based Sorting Algorithms on GPUs",
      "authors": ["Ben Karsin", "Volker Weichert", "Henri Casanova", "John Iacono", "Nodari Sitchinava"],
      "abstract": "We study the relationship between memory accesses, bank conflicts, thread multiplicity (also known as over-subscription) and instruction-level parallelism in comparison-based sorting algorithms for Graphics Processing Units (GPUs). We experimentally validate a proposed formula that relates these parameters with asymptotic analysis of the number of memory accesses by an algorithm. Using this formula we analyze and compare several GPU sorting algorithms, identifying key performance bottlenecks in each one of them. Based on this analysis we propose a GPU-efficient multiway merge-sort algorithm, GPU-MMS, which minimizes or eliminates these bottlenecks and balances various limiting factors for specific hardware. We realize an implementation of GPU-MMS and compare it to sorting algorithm implementations in state-of-the-art GPU libraries on three GPU architectures. Despite these library implementations being highly optimized, we find that GPU-MMS outperforms them by an average of 21% for random integer inputs and 14% for random key-value pairs",
      "area": ["GPU", "sorting", "mergesort", "bank conflicts", "I/O efficiency"],
      "link": "https://henricasanova.github.io/files/papers/karsin_ics2018.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "Checkpointing Workflows for Fail-Stop Errors",
      "authors": ["Li Han", "Louis-Claude Canon", "Henri Casanova", "Yves Robert", "Frederic Vivien"],
      "abstract": "We consider the problem of orchestrating the execution of workflow applications structured as Directed Acyclic Graphs (DAGs) on parallel computing platforms that are subject to fail-stop failures. The objective is to minimize expected overall execution time, or makespan. A solution to this problem consists of a schedule of the workflow tasks on the available processors and of a decision of which application data to checkpoint to stable storage, so as to mitigate the impact of processor failures. To address this challenge, we consider a restricted class of graphs, Minimal Series-Parallel Graphs (M-SPGS), which is relevant to many real-world workflow applications. For this class of graphs, we propose a recursive list-scheduling algorithm that exploits the M-SPG structure to assign sub-graphs to individual processors, and uses dynamic programming to decide how to checkpoint these sub-graphs. We assess the performance of our algorithm for production workflow configurations, comparing it to an approach in which all application data is checkpointed and an approach in which no application data is checkpointed. Results demonstrate that our algorithm outperforms both the former approach, because of lower checkpointing overhead, and the latter approach, because of better resilience to failures.",
      "area": ["Workflow", "checkpoint", "fail-stop error", "resilience"],
      "link": "https://henricasanova.github.io/files/papers/han_tc2018.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "Computing the expected makespan of task graphs in the presence of silent errors",
      "authors": ["Henri Casanova", "Julien Herrmann", "Yves Robert"],
      "abstract": "Applications structured as Directed Acyclic Graphs (DAGs) of tasks occur in many domains, including popular scientific workflows. DAG scheduling has thus received an enormous amount of attention. Many of the popular DAG scheduling heuristics make scheduling decisions based on path lengths. At large scale compute platforms are subject to various types of failures with non-negligible probabilities of occurrence. Failures that have recently received increased attention are “silent errors,” which cause data corruption. Tolerating silent errors is done by checking the validity of computed results and possibly re-executing a task from scratch. The execution time of a task then becomes a random variable, and so do path lengths in a DAG. Unfortunately, computing the expected makespan of a DAG (and equivalently computing expected path lengths in a DAG) is a computationally difficult problem. Consequently, designing effective scheduling heuristics in this context is challenging. In this work we propose an algorithm that computes a first order approximation of the expected makespan of a DAG when tasks are subject to silent errors. We find that our proposed approximation outperforms previously proposed approaches both in terms of approximation error and of speed.",
      "area": ["Directed Acyclic Graphs", "makespan", "workflow"],
      "link": "https://henricasanova.github.io/files/papers/herrmann_parco2018.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "A Case for Uni-Directional Network Topologies in Large-Scale Clusters",
      "authors": ["Michihiro Koibuchi", "Tomohiro Totoki", "Hiroki Matsutani", "Hideharu Amano", "Fabien Chaix", "Ikki Fujiwara",  "Henri Casanova"],
      "abstract": "Designing low-latency network topologies of switches is a key objective for next-generation large-scale clusters. Low latency is preconditioned on low hop counts, but existing network topologies have hop counts much larger than theoretical lower bounds. To alleviate this problem, we propose building network topologies based on uni-directional graphs that are known to have hop counts close to theoretical lower bounds. A practical difficulty with uni-directional topologies is switch-by-switch flow control, which we resolve by using hot-potato routing. Cycle-accurate network simulation experiments for various traffic patterns on uni-directional topologies show that hot-potato routing achieves performance comparable to that of conventional deadlock-free routing. Similar experiments are used to compare several uni-directional topologies to bi-directional topologies, showing that the former achieve significantly lower latency and higher throughput. We quantify end-to-end application performance for parallel application benchmarks via discrete-even simulation, showing that uni-directional topologies can lead to large application performance improvements over their bi-directional counterparts. Finally, we discuss practical issues for uni-directional topologies such as cabling complexity and cost, power consumption, and soft-error tolerance. Our results make a compelling case for considering uni-directional topologies for upcoming large-scale clusters.",
      "area": ["HPC clusters", "interconnection networks", "uni-directional network topologies", "hot-potato routing"],
      "link": "https://henricasanova.github.io/files/papers/koibuchi_cluster2017.pdf",
      "owner": "abc@foo.com"
    },
    {
      "title": "Towards Ideal Hop Counts in Interconnection Networks with Arbitrary Size",
      "authors": ["Michihiro Koibuchi", "Ikki Fujiwara", "Fabien Chaix", "Henri Casanova"],
      "abstract": "Designing low-latency network topologies of switches is a key objective for next-generation parallel computing platforms. Low latency is preconditioned on low hop counts, but existing network topologies have hop counts much larger than theoretical lower bounds. The degree diameter problem (DDP) has been studied for decades and consists in generating the largest possible graph given degree and diameter constraints, striving to approach theoretical upper bounds. To generate network topologies with low hop counts we propose using best known DDP solutions as starting points for generating topologies of arbitrary size. Using discrete-event simulation, we quantify the performance of representative parallel applications when executed on our proposed topologies, on previously proposed fully random topologies, and on a classical non-random topology.",
      "area": ["interconnection networks", "network topology", "diameter", "random topology"],
      "link": "https://henricasanova.github.io/files/papers/koibuchi_candar2016.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Computing the expected makespan of task graphs in the presence of silent errors",
      "authors": ["Henri Casanova", "Julien Herrmann", "Yves Robert"],
      "abstract": "Applications structured as Directed Acyclic Graphs (DAGs) of tasks correspond to a general model of parallel computation and thus arise in most domains (often as “scientific workflows”). A key question that is the subject of large literature is that of DAG scheduling, for which several list-scheduling heuristics have been proposed and shown to be effective in practice. Many of these heuristics make scheduling decisions based on path lengths in the DAG. At large scale, however, compute platforms and thus tasks are subject to various types of failures with no longer negligible probabilities of occurrence. Failures that have recently received increasing attention are “silent errors,” which cause a task to produce incorrect results even though it ran to completion. Tolerating silent errors is done by checking the validity of the results and reexecuting the task from scratch in case of an invalid result. The execution time of a task then becomes a random variable, and so are path lengths. Unfortunately, computing the expected makespan of a DAG (and equivalently computing expected path lengths in a DAG) is a computationally difficult problem. Consequently, designing effective scheduling heuristics is preconditioned on computing accurate approximations of the expected makespan. In this work we propose an algorithm that computes a first order approximation of the expected makespan of a DAG when tasks are subject to silent errors. We compare our proposed approximation to previously proposed such approximations for three classes of application graphs from the field of numerical linear algebra. Our evaluations quantify approximation error with respect to a ground truth computed via a brute-force Monte Carlo method. We find that our proposed approximation outperforms previously proposed approaches, and in particular leads to large reductions in approximation error for low (and realistic) failure rates.",
      "area": ["DAG scheduling", "makespan", "silent errors"],
      "link": "https://henricasanova.github.io/files/papers/casanova_p2s22016.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Checkpointing Strategies for Scheduling Computational Workflows",
      "authors": ["Guillaume Aupy", "Anne Benoit", "Henri Casanova", "Yves Robert"],
      "abstract": "We study the scheduling of computational workflows on compute resources that experience exponentially distributed failures. When a failure occurs, rollback and recovery is used to resume the execution from the last checkpointed state. The scheduling problem is to minimize the expected execution time by deciding in which order to execute the tasks in the workflow and deciding for each task whether to checkpoint it or not after it completes. We give a polynomialtime optimal algorithm for fork DAGs (Directed Acyclic Graphs) and show that the problem is NP-complete with join DAGs. We also investigate the complexity of the simple case in which no task is checkpointed. Our main result is a polynomial-time algorithm to compute the expected execution time of a workflow, with a given task execution order and specified to-be-checkpointed tasks. Using this algorithm as a basis, we propose several heuristics for solving the scheduling problem. We evaluate these heuristics for representative workflow configurations.",
      "area": ["checkpointing", "scheduling", "workflow", "fault-tolerance", "reliability"],
      "link": "https://henricasanova.github.io/files/papers/aupy_ijnc2016.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Distance Threshold Similarity Searches: Efficient Trajectory Indexing on the GPU",
      "authors": ["Michael Gowanlock", "Henri Casanova"],
      "abstract": "Applications in many domains perform searches over datasets that contain moving object trajectories. A common class of searches are similarity searches that attempt to identify trajectories with similar characteristics. In this work, we focus on the distance threshold similarity search that finds all trajectories within a given distance of a query trajectory over a time interval. This search involves large numbers of Euclidean moving distance calculations, thus making it a good candidate for execution on manycore platforms such as GPUs. However, low search response time is preconditioned on efficient indexing of trajectory data. We propose three indexing schemes designed for the GPU, with spatial, temporal and spatiotemporal selectivity. These schemes differ significantly from traditional tree-based indexing schemes that have been previously proposed for CPU executions. We evaluate implementations of our proposed indexing schemes using two synthetic and one realworld astrophysics dataset, showing under which conditions each scheme achieves high performance. Our broad finding is that a GPU implementation, provided an appropriate indexing scheme is used, can outperform a multithreaded CPU implementation that uses a state-of-the-art index tree. In particular, the performance improvement is large for regimes that are relevant for classes of real-world applications, thereby demonstrating that the GPU is an attractive platform for searching and processing moving object trajectories.",
      "area": ["GPU", "trajectory indexing", "distance threshold similarity"],
      "link": "https://henricasanova.github.io/files/papers/gowanlock_tpds2016.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Toward More Scalable Off-line Simulations of MPI Applications",
      "authors": ["Henri Casanova", "Anshul Gupta", "Frédéric Suter"],
      "abstract": "The off-line (or post-mortem) analysis of execution event traces is a popular approach to understand the performance of HPC applications that use the message passing paradigm. Combining this analysis with simulation makes it possible to \"replay\" the application execution to explore \"what if?\" scenarios, e.g., assessing application performance in a range of (hypothetical) execution environments. However, such off-line analysis faces scalability issues for acquiring, storing, or replaying large event traces. We first present two previously proposed and complementary frameworks for off-line replaying of MPI application event traces, each with its own objectives and limitations. We then describe how these frameworks can be combined so as to capitalize on their respective strengths while alleviating several of their limitations. We claim that the combined framework affords levels of scalability that are beyond that achievable by either one of the two individual frameworks. We evaluate this framework to illustrate the benefits of the proposed combination for a more scalable off-line analysis of MPI applications.",
      "area": ["MPI", "HPC", "Frameworks"],
      "link": "https://henricasanova.github.io/files/papers/casanova_ppl2015.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Efficient Batched Predecessor Search in Shared Memory on GPUs",
      "authors": ["Ben Karsin", "Henri Casanova", "Nodari Sitchinava"],
      "abstract": "Many-core Graphics Processing Units (GPUs) are being used for general-purpose computing. However, due to architectural features, for many problems it is challenging to design parallel algorithms that exploit the full compute power of GPUs. Among these features is the memory design. Although the issue of coalesced global memory access has been documented and studied extensively, another important architectural feature is the organization of shared memory into banks. The study of how bank conflicts impact algorithm performance has only recently begun to receive attention. In this work we study the predecessor search algorithm and the effects of bank conflicts on its execution time. Via complexity analysis we show that bank conflicts cause significant loss in parallelism for a naive algorithm. We then propose two improved algorithms: one that eliminates bank conflicts altogether but that uses a work-inefficient linear search, and one that is work-optimal but that experiences a limited number of bank conflicts. We develop GPU implementations of these algorithms and present experimental results obtained on realworld hardware. These results validate our theoretical analysis of the naive algorithm and allow us to assess the performance of our algorithms in practice. Although both our improved algorithms outperform the naive algorithm, our main experimental finding is that our conflict-limited algorithm provides a larger performance gain.",
      "area": ["GPU", "predecessor search algorithm", "bank conflicts", "complexity analysis"],
      "link": "https://henricasanova.github.io/files/papers/karsin_hipc2015.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "On the Impact of Process Replication on Executions of Large-Scale Parallel Applications with Coordinated Checkpointing",
      "authors": ["Henri Casanova", "Yves Robert", "Frédéric Vivien", "Dounia Zaidouni"],
      "abstract": "Processor failures in post-petascale parallel computing platforms are common occurrences. The traditional fault-tolerance solution, checkpoint-rollback-recovery, severely limits parallel efficiency. One solution is to replicate application processes so that a processor failure does not necessarily imply an application failure. Process replication, combined with checkpoint-rollbackrecovery, has been recently advocated. We first derive novel theoretical results for Exponential failure distributions, namely exact values for the Mean Number of Failures To Interruption and the Mean Time To Interruption. We then extend these results to arbitrary failure distributions, obtaining closed-form solutions for Weibull distributions. Finally, we evaluate process replication in simulation using both synthetic and real-world failure traces so as to quantify average application makespan. One interesting result from these experiments is that, when process replication is used, application performance is not sensitive to the checkpointing period, provided that that period is within a large neighborhood of the optimal period. More generally, our empirical results make it possible to identify regimes in which process replication is beneficial.",
      "area": ["Fault-tolerance", "parallel computing", "checkpoint", "rollback-recovery", "process replication"],
      "link": "https://henricasanova.github.io/files/papers/casanova_fgcs2015.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Indexing of Spatiotemporal Trajectories for Efficient Distance Threshold Similarity Searches on the GPU",
      "authors": ["Michael Gowanlock", "Henri Casanova"],
      "abstract": "Applications in many domains search moving object trajectory databases. The distance threshold search finds all trajectories within a given distance of a query trajectory. We develop three GPU distance threshold search implementations that use indexing techniques significantly different from those used in CPU implementations. We determine experimentally under which conditions each approach performs well using one real-world astrophysics dataset and two synthetic datasets. Overall, we find that the GPU is an attractive technology for a broad range of relevant trajectory database scenarios.",
      "area": ["Distance threshold similarity search", "GPU", "moving object databases", "query optimization"],
      "link": "https://henricasanova.github.io/files/papers/gowanlock_ipdps2015.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Simulation of MPI Applications with Time-Independent Traces",
      "authors": ["Henri Casanova", "Frédéric Desprez", "George S. Markomanolis", "Frédéric Suter"],
      "abstract": "Analyzing and understanding the performance behavior of parallel applications on parallel computing platforms is a long-standing concern in the High Performance Computing community. When the targeted platforms are not available, simulation is a reasonable approach to obtain objective performance indicators and explore various hypothetical scenarios. In the context of applications implemented with the Message Passing Interface, two simulation methods have been proposed, on-line simulation and off-line simulation, both with their own drawbacks and advantages. In this work we present an off-line simulation framework, i.e., one that simulates the execution of an application based on event traces obtained from an actual execution. The main novelty of this work, when compared to previously proposed off-line simulators, is that traces that drive the simulation can be acquired on large, distributed, heterogeneous, and non-dedicated platforms. As a result the scalability of trace acquisition is increased, which is achieved by enforcing that traces contain no time-related information. Moreover, our framework is based on an state-of-the-art scalable, fast, and validated simulation kernel. We introduce the notion of performing off-line simulation from time-independent traces, propose and evaluate several trace acquisition strategies, describe our simulation framework, and assess its quality in terms of trace acquisition scalability, simulation accuracy, and simulation time. Copyright c 2014 John Wiley & Sons, Ltd.",
      "area": ["Performance prediction", "MPI", "Simulation"],
      "link": "https://henricasanova.github.io/files/papers/desprez_ccpe2014.pdf",
      "owner": "xyz@foo.com"
    },
    {
      "title": "Selecting Linear Algebra Kernel Composition Using Response Time Prediction",
      "authors": ["Aurélie Hurault", "Kyungim Baek", "Henri Casanova", "", "", "", "", "", ""],
      "abstract": "Numerical linear algebra libraries provide many kernels that can be composed to perform complex computations. For a given computation there is typically a large number of functionally equivalent kernel compositions. Some of these compositions achieve better response times than others for particular data and when executed on a particular computer architecture. Previous research provides methods to enumerate (a subset of) these kernel compositions. In this work we study the problem of determining the composition that yields the lowest response time. Our approach is based on a response time prediction for each candidate combination. While this prediction could in principle be obtained using analytical and/or empirical performance models, developing accurate such models is known to be challenging. Instead, we define a feature space that captures salient properties of kernel combinations and predict response time using supervised machine learning. We experiment with a standard set of machine learning algorithms and identify an effective algorithm for our kernel composition selection problem. Using this algorithm our approach widely outperforms the strategy that would consist in always using the simplest kernel composition, and is often close to the fastest kernel compositions among those evaluated. We quantify the potential benefit of our approach if it were to be implemented as part of an interactive computational tool. We find that although the potential benefit is substantial, a limiting factor is the kernel composition enumeration overhead. Copyright c 2014 John Wiley & Sons, Ltd.",
      "area": ["Numerical linear algebra", "kernel compositions", "performance prediction", "machine learning"],
      "link": "https://henricasanova.github.io/files/papers/hurault_spe2015.pdf",
      "owner": "xyz@foo.com"
    }
  ]
}
